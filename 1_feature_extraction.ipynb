{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "1-feature-extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4188e1f5df44db3998484e6d2b92c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6164a0fdac9d450ba8e53fbc79072d00",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3afd51b962d44ca088c08d5d88cff9c3",
              "IPY_MODEL_0689f93e738049eca0ef99a64193c7cd"
            ]
          }
        },
        "6164a0fdac9d450ba8e53fbc79072d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "width": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "overflow": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "overflow_y": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "align_self": null,
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3afd51b962d44ca088c08d5d88cff9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81608fdf1953480dac5aff8a3566392c",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "orientation": "horizontal",
            "min": 0,
            "bar_style": "success",
            "max": 8677,
            "_model_name": "IntProgressModel",
            "_model_module_version": "1.5.0",
            "value": 8677,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b67a4f9fa0b47f18ab15c2a4f21357a",
            "description": ""
          }
        },
        "0689f93e738049eca0ef99a64193c7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5e7cdd876cd14c2b982f3e2d180f0db8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 8677/8677 [1:03:56&lt;00:00,  2.48it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_82c8e10f269e43d3ae040cad11b2657f"
          }
        },
        "81608fdf1953480dac5aff8a3566392c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b67a4f9fa0b47f18ab15c2a4f21357a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "width": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "overflow": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "overflow_y": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "align_self": null,
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e7cdd876cd14c2b982f3e2d180f0db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "82c8e10f269e43d3ae040cad11b2657f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "width": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "overflow": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "overflow_y": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "align_self": null,
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEsoTZuC8Nc",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction\n",
        "\n",
        "We will extract features from pretrained models like VGG-16, VGG-19, ResNet-50, InceptionV3 and MobileNet and benchmark them using the Caltech101 dataset.\n",
        "\n",
        "## Dataset:\n",
        "\n",
        "We will download the Caltech101 dataset in the `data` directory of the repo.\n",
        "\n",
        "```\n",
        "$ curl http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz --output caltech101.tar.gz\n",
        "\n",
        "$ tar -xvzf caltech101.tar.gz\n",
        "\n",
        "$ mv 101_ObjectCategories datasets/caltech101\n",
        "```\n",
        "There is a 102nd category called ‘BACKGROUND_Google’ consisting of random images not contained in the first 101 categories, which needs to be deleted before we start experimenting. \n",
        "\n",
        "```\n",
        "$ rm -rf datasets/caltech101/BACKGROUND_Google\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uav2br_FC8Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir -p ../../datasets\n",
        "# !curl http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz --output ../../datasets/caltech101.tar.gz\n",
        "!tar -xvzf ../../datasets/caltech101.tar.gz --directory ../../datasets\n",
        "!mv ../../datasets/101_ObjectCategories ../../datasets/caltech101\n",
        "!rm -rf ../../datasets/caltech101/BACKGROUND_Google"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "FmGwOYmjC8Nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TXYrVR-C8Nv",
        "colab_type": "text"
      },
      "source": [
        "We will define a helper function that allows us to choose any pretrained model with all the necessary details for our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fYnaLzzC8Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_picker(name):\n",
        "    if (name == 'vgg16'):\n",
        "        model = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'vgg19'):\n",
        "        model = VGG19(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'mobilenet'):\n",
        "        model = MobileNet(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(224, 224, 3),\n",
        "                          pooling='max',\n",
        "                          depth_multiplier=1,\n",
        "                          alpha=1)\n",
        "    elif (name == 'inception'):\n",
        "        model = InceptionV3(weights='imagenet',\n",
        "                            include_top=False,\n",
        "                            input_shape=(224, 224, 3),\n",
        "                            pooling='max')\n",
        "    elif (name == 'resnet'):\n",
        "        model = ResNet50(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                        pooling='max')\n",
        "    elif (name == 'xception'):\n",
        "        model = Xception(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                         pooling='max')\n",
        "    else:\n",
        "        print(\"Specified model not available\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kV_rF4tC8N2",
        "colab_type": "text"
      },
      "source": [
        "Now, let's put our function to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2kVkbX3C8N3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "bd8fce4d-18c9-45ff-c37c-265511f3b6e1"
      },
      "source": [
        "model_architecture = 'resnet'\n",
        "model = model_picker(model_architecture)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 2s 0us/step\n",
            "94781440/94765736 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFtRfgWVC8OB",
        "colab_type": "text"
      },
      "source": [
        "Let's define a function to extract image features given an image and a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FFHGunqC8OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(img_path, model):\n",
        "    input_shape = (224, 224, 3)\n",
        "    img = image.load_img(img_path,\n",
        "                         target_size=(input_shape[0], input_shape[1]))\n",
        "    img_array = image.img_to_array(img)\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    features = model.predict(preprocessed_img)\n",
        "    flattened_features = features.flatten()\n",
        "    normalized_features = flattened_features / norm(flattened_features)\n",
        "    return normalized_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Il6iz6C8OP",
        "colab_type": "text"
      },
      "source": [
        "Let's see the feature length the model generates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tP5oOHXC8OQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb6d6758-bd05-49dd-bfb5-f7b173a88bd2"
      },
      "source": [
        "features = extract_features('../../sample-images/cat.jpg', model)\n",
        "print(len(features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaTdT7zCC8OW",
        "colab_type": "text"
      },
      "source": [
        "Now, we will see how much time it takes to extract features of one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riI2w5DWC8OY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a5bbc03-44de-475a-d779-6cb35c5d6271"
      },
      "source": [
        "%timeit features = extract_features('../../sample-images/cat.jpg', model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 208 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVqr1PL1C8Oo",
        "colab_type": "text"
      },
      "source": [
        "The time taken to extract features is dependent on a few factors such as image size, computing power etc. A better benchmark would be running the network over an entire dataset. A simple change to the existing code will allow this.\n",
        "\n",
        "Let's make a handy function to recursively get all the image files under a root directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrlRDS9EC8Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "\n",
        "def get_file_list(root_dir):\n",
        "    file_list = []\n",
        "    for root, directories, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if any(ext in filename for ext in extensions):\n",
        "                file_list.append(os.path.join(root, filename))\n",
        "    return file_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnIWj-w0C8Ot",
        "colab_type": "text"
      },
      "source": [
        "Now, let's run the extraction over the entire dataset and time it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM04-QlFC8Ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a4188e1f5df44db3998484e6d2b92c64",
            "6164a0fdac9d450ba8e53fbc79072d00",
            "3afd51b962d44ca088c08d5d88cff9c3",
            "0689f93e738049eca0ef99a64193c7cd",
            "81608fdf1953480dac5aff8a3566392c",
            "4b67a4f9fa0b47f18ab15c2a4f21357a",
            "5e7cdd876cd14c2b982f3e2d180f0db8",
            "82c8e10f269e43d3ae040cad11b2657f"
          ]
        },
        "outputId": "7a080c04-49f8-4e69-d941-f701cfef8f33"
      },
      "source": [
        "# path to the your datasets\n",
        "root_dir = '../../datasets/caltech101'\n",
        "filenames = sorted(get_file_list(root_dir))\n",
        "\n",
        "feature_list = []\n",
        "for i in tqdm_notebook(range(len(filenames))):\n",
        "    feature_list.append(extract_features(filenames[i], model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4188e1f5df44db3998484e6d2b92c64",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=8677), HTML(value=u'')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DWVkUDjC8Oy",
        "colab_type": "text"
      },
      "source": [
        "Now let's try the same with the Keras Image Generator functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkPufz-_C8Oz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57b8b8bb-44a2-49a4-e7ab-d796d69f24c8"
      },
      "source": [
        "batch_size = 64\n",
        "root_dir = '../../datasets/caltech101'\n",
        "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "generator = datagen.flow_from_directory(root_dir,\n",
        "                                        target_size=(224, 224),\n",
        "                                        batch_size=batch_size,\n",
        "                                        class_mode=None,\n",
        "                                        shuffle=False)\n",
        "\n",
        "num_images = len(generator.filenames)\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\n",
        "\n",
        "start_time = time.time()\n",
        "feature_list = []\n",
        "feature_list = model.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8677 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYwDHXu8C8O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, features in enumerate(feature_list):\n",
        "    feature_list[i] = features / norm(features)\n",
        "\n",
        "feature_list = feature_list.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh6-x23EC8O7",
        "colab_type": "text"
      },
      "source": [
        "Let's save the features as intermediate files to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7K-xdFpC8O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = [root_dir + '/' + s for s in generator.filenames]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "obdr-_vjC8PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(generator.classes, open('./data/class_ids-caltech101.pickle',\n",
        "                                    'wb'))\n",
        "pickle.dump(filenames, open('./data/filenames-caltech101.pickle', 'wb'))\n",
        "pickle.dump(\n",
        "    feature_list,\n",
        "    open('./data/features-caltech101-' + model_architecture + '.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iypCLPywC8PF",
        "colab_type": "text"
      },
      "source": [
        "Let's train a finetuned model as well and save the features for that as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "migRfhemC8PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_SAMPLES = 8677\n",
        "NUM_CLASSES = 101\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOpS3wnaC8PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   zoom_range=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6hifmqsC8PO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed26e295-571b-4edd-8987-ed9422e09dd4"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(root_dir,\n",
        "                                                    target_size=(IMG_WIDTH,\n",
        "                                                                 IMG_HEIGHT),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=12345,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8677 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMCFBJkyC8PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_maker():\n",
        "    base_model = ResNet50(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER3YmqE3C8PY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "ab4b0753-028e-4f9b-9e02-d23008fecc2a"
      },
      "source": [
        "model_finetuned = model_maker()\n",
        "model_finetuned.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
        "              metrics=['acc'])\n",
        "model_finetuned.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / batch_size),\n",
        "    epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0822 07:50:13.373927 139844610570112 deprecation.py:323] From <ipython-input-30-3632d4ca8dfa>:8: fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "W0822 07:50:14.169017 139844610570112 data_adapter.py:1091] sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train for 136.0 steps\n",
            "Epoch 1/10\n",
            "136/136 [==============================] - 1732s 13s/step - loss: 3.0893 - acc: 0.3323\n",
            "Epoch 2/10\n",
            "136/136 [==============================] - 1744s 13s/step - loss: 1.9841 - acc: 0.5220\n",
            "Epoch 3/10\n",
            "136/136 [==============================] - 1731s 13s/step - loss: 1.5744 - acc: 0.5943\n",
            "Epoch 4/10\n",
            "136/136 [==============================] - 1777s 13s/step - loss: 1.3537 - acc: 0.6425\n",
            "Epoch 5/10\n",
            "136/136 [==============================] - 1804s 13s/step - loss: 1.1873 - acc: 0.6728\n",
            "Epoch 6/10\n",
            "136/136 [==============================] - 1797s 13s/step - loss: 1.0848 - acc: 0.6955\n",
            "Epoch 7/10\n",
            "136/136 [==============================] - 1793s 13s/step - loss: 1.0382 - acc: 0.7106\n",
            "Epoch 8/10\n",
            "136/136 [==============================] - 1773s 13s/step - loss: 0.9659 - acc: 0.7291\n",
            "Epoch 9/10\n",
            "136/136 [==============================] - 1766s 13s/step - loss: 0.9174 - acc: 0.7372\n",
            "Epoch 10/10\n",
            "136/136 [==============================] - 1765s 13s/step - loss: 0.8837 - acc: 0.7410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2fa2629950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOgdi5MPC8Pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_finetuned.save('./data/model-finetuned.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO7aHSYhC8Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "feature_list_finetuned = []\n",
        "feature_list_finetuned = model_finetuned.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()\n",
        "\n",
        "for i, features_finetuned in enumerate(feature_list_finetuned):\n",
        "    feature_list_finetuned[i] = features_finetuned / norm(features_finetuned)\n",
        "\n",
        "feature_list = feature_list_finetuned.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2duAQ5Y0C8Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(\n",
        "    feature_list,\n",
        "    open('./data/features-caltech101-resnet-finetuned.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}